{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da5cdb3d",
   "metadata": {
    "id": "da5cdb3d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50708618",
   "metadata": {
    "id": "50708618"
   },
   "outputs": [],
   "source": [
    "iris=load_iris()\n",
    "values=iris.data\n",
    "target=iris.target\n",
    "\n",
    "ss=StandardScaler()\n",
    "values=ss.fit_transform(values)\n",
    "\n",
    "values_train,values_test,target_train,target_test=train_test_split(values,target,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64682312",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64682312",
    "outputId": "11dcd84b-937c-46bd-941b-ce8d3ab25124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.37327721\n",
      "Iteration 2, loss = 1.14000532\n",
      "Iteration 3, loss = 1.10212588\n",
      "Iteration 4, loss = 1.12580967\n",
      "Iteration 5, loss = 1.13543090\n",
      "Iteration 6, loss = 1.11634010\n",
      "Iteration 7, loss = 1.08992140\n",
      "Iteration 8, loss = 1.07405482\n",
      "Iteration 9, loss = 1.06739258\n",
      "Iteration 10, loss = 1.06036207\n",
      "Iteration 11, loss = 1.04818827\n",
      "Iteration 12, loss = 1.03145071\n",
      "Iteration 13, loss = 1.01126359\n",
      "Iteration 14, loss = 0.98712687\n",
      "Iteration 15, loss = 0.95769554\n",
      "Iteration 16, loss = 0.92216180\n",
      "Iteration 17, loss = 0.88097308\n",
      "Iteration 18, loss = 0.83571983\n",
      "Iteration 19, loss = 0.78860483\n",
      "Iteration 20, loss = 0.74196855\n",
      "Iteration 21, loss = 0.69796419\n",
      "Iteration 22, loss = 0.65825248\n",
      "Iteration 23, loss = 0.62376968\n",
      "Iteration 24, loss = 0.59471423\n",
      "Iteration 25, loss = 0.57073787\n",
      "Iteration 26, loss = 0.55119926\n",
      "Iteration 27, loss = 0.53536402\n",
      "Iteration 28, loss = 0.52252168\n",
      "Iteration 29, loss = 0.51204148\n",
      "Iteration 30, loss = 0.50339286\n",
      "Iteration 31, loss = 0.49614545\n",
      "Iteration 32, loss = 0.48995762\n",
      "Iteration 33, loss = 0.48456094\n",
      "Iteration 34, loss = 0.47974482\n",
      "Iteration 35, loss = 0.47534347\n",
      "Iteration 36, loss = 0.47122529\n",
      "Iteration 37, loss = 0.46728470\n",
      "Iteration 38, loss = 0.46343582\n",
      "Iteration 39, loss = 0.45960778\n",
      "Iteration 40, loss = 0.45574099\n",
      "Iteration 41, loss = 0.45178438\n",
      "Iteration 42, loss = 0.44769303\n",
      "Iteration 43, loss = 0.44342671\n",
      "Iteration 44, loss = 0.43894902\n",
      "Iteration 45, loss = 0.43422764\n",
      "Iteration 46, loss = 0.42923521\n",
      "Iteration 47, loss = 0.42395036\n",
      "Iteration 48, loss = 0.41835799\n",
      "Iteration 49, loss = 0.41244812\n",
      "Iteration 50, loss = 0.40621353\n",
      "Iteration 51, loss = 0.39964734\n",
      "Iteration 52, loss = 0.39274199\n",
      "Iteration 53, loss = 0.38549032\n",
      "Iteration 54, loss = 0.37788744\n",
      "Iteration 55, loss = 0.36993122\n",
      "Iteration 56, loss = 0.36162000\n",
      "Iteration 57, loss = 0.35294858\n",
      "Iteration 58, loss = 0.34390498\n",
      "Iteration 59, loss = 0.33447045\n",
      "Iteration 60, loss = 0.32462354\n",
      "Iteration 61, loss = 0.31434754\n",
      "Iteration 62, loss = 0.30364000\n",
      "Iteration 63, loss = 0.29252247\n",
      "Iteration 64, loss = 0.28104909\n",
      "Iteration 65, loss = 0.26931170\n",
      "Iteration 66, loss = 0.25743976\n",
      "Iteration 67, loss = 0.24559426\n",
      "Iteration 68, loss = 0.23395772\n",
      "Iteration 69, loss = 0.22272321\n",
      "Iteration 70, loss = 0.21208176\n",
      "Iteration 71, loss = 0.20220264\n",
      "Iteration 72, loss = 0.19320520\n",
      "Iteration 73, loss = 0.18513228\n",
      "Iteration 74, loss = 0.17793934\n",
      "Iteration 75, loss = 0.17150545\n",
      "Iteration 76, loss = 0.16566191\n",
      "Iteration 77, loss = 0.16022826\n",
      "Iteration 78, loss = 0.15504420\n",
      "Iteration 79, loss = 0.14999018\n",
      "Iteration 80, loss = 0.14499482\n",
      "Iteration 81, loss = 0.14003146\n",
      "Iteration 82, loss = 0.13510850\n",
      "Iteration 83, loss = 0.13025730\n",
      "Iteration 84, loss = 0.12552062\n",
      "Iteration 85, loss = 0.12094312\n",
      "Iteration 86, loss = 0.11656446\n",
      "Iteration 87, loss = 0.11241495\n",
      "Iteration 88, loss = 0.10851363\n",
      "Iteration 89, loss = 0.10486806\n",
      "Iteration 90, loss = 0.10147543\n",
      "Iteration 91, loss = 0.09832452\n",
      "Iteration 92, loss = 0.09539796\n",
      "Iteration 93, loss = 0.09267461\n",
      "Iteration 94, loss = 0.09013182\n",
      "Iteration 95, loss = 0.08774742\n",
      "Iteration 96, loss = 0.08550124\n",
      "Iteration 97, loss = 0.08337621\n",
      "Iteration 98, loss = 0.08135882\n",
      "Iteration 99, loss = 0.07943918\n",
      "Iteration 100, loss = 0.07761053\n",
      "Iteration 101, loss = 0.07586856\n",
      "Iteration 102, loss = 0.07421062\n",
      "Iteration 103, loss = 0.07263491\n",
      "Iteration 104, loss = 0.07113985\n",
      "Iteration 105, loss = 0.06972359\n",
      "Iteration 106, loss = 0.06838372\n",
      "Iteration 107, loss = 0.06711719\n",
      "Iteration 108, loss = 0.06592027\n",
      "Iteration 109, loss = 0.06478874\n",
      "Iteration 110, loss = 0.06371805\n",
      "Iteration 111, loss = 0.06270355\n",
      "Iteration 112, loss = 0.06174066\n",
      "Iteration 113, loss = 0.06082509\n",
      "Iteration 114, loss = 0.05995294\n",
      "Iteration 115, loss = 0.05912075\n",
      "Iteration 116, loss = 0.05832556\n",
      "Iteration 117, loss = 0.05756486\n",
      "Iteration 118, loss = 0.05683652\n",
      "Iteration 119, loss = 0.05613873\n",
      "Iteration 120, loss = 0.05546990\n",
      "Iteration 121, loss = 0.05482861\n",
      "Iteration 122, loss = 0.05421350\n",
      "Iteration 123, loss = 0.05362326\n",
      "Iteration 124, loss = 0.05305657\n",
      "Iteration 125, loss = 0.05251215\n",
      "Iteration 126, loss = 0.05198869\n",
      "Iteration 127, loss = 0.05148492\n",
      "Iteration 128, loss = 0.05099963\n",
      "Iteration 129, loss = 0.05053163\n",
      "Iteration 130, loss = 0.05007985\n",
      "Iteration 131, loss = 0.04964328\n",
      "Iteration 132, loss = 0.04922101\n",
      "Iteration 133, loss = 0.04881224\n",
      "Iteration 134, loss = 0.04841622\n",
      "Iteration 135, loss = 0.04803229\n",
      "Iteration 136, loss = 0.04765986\n",
      "Iteration 137, loss = 0.04729836\n",
      "Iteration 138, loss = 0.04694730\n",
      "Iteration 139, loss = 0.04660618\n",
      "Iteration 140, loss = 0.04627456\n",
      "Iteration 141, loss = 0.04595199\n",
      "Iteration 142, loss = 0.04563806\n",
      "Iteration 143, loss = 0.04533236\n",
      "Iteration 144, loss = 0.04503453\n",
      "Iteration 145, loss = 0.04474419\n",
      "Iteration 146, loss = 0.04446100\n",
      "Iteration 147, loss = 0.04418466\n",
      "Iteration 148, loss = 0.04391486\n",
      "Iteration 149, loss = 0.04365132\n",
      "Iteration 150, loss = 0.04339379\n",
      "Iteration 151, loss = 0.04314202\n",
      "Iteration 152, loss = 0.04289579\n",
      "Iteration 153, loss = 0.04265490\n",
      "Iteration 154, loss = 0.04241915\n",
      "Iteration 155, loss = 0.04218834\n",
      "Iteration 156, loss = 0.04196230\n",
      "Iteration 157, loss = 0.04174088\n",
      "Iteration 158, loss = 0.04152389\n",
      "Iteration 159, loss = 0.04131120\n",
      "Iteration 160, loss = 0.04110266\n",
      "Iteration 161, loss = 0.04089813\n",
      "Iteration 162, loss = 0.04069748\n",
      "Iteration 163, loss = 0.04050058\n",
      "Iteration 164, loss = 0.04030732\n",
      "Iteration 165, loss = 0.04011758\n",
      "Iteration 166, loss = 0.03993125\n",
      "Iteration 167, loss = 0.03974823\n",
      "Iteration 168, loss = 0.03956842\n",
      "Iteration 169, loss = 0.03939173\n",
      "Iteration 170, loss = 0.03921806\n",
      "Iteration 171, loss = 0.03904734\n",
      "Iteration 172, loss = 0.03887947\n",
      "Iteration 173, loss = 0.03871438\n",
      "Iteration 174, loss = 0.03855199\n",
      "Iteration 175, loss = 0.03839223\n",
      "Iteration 176, loss = 0.03823502\n",
      "Iteration 177, loss = 0.03808029\n",
      "Iteration 178, loss = 0.03792799\n",
      "Iteration 179, loss = 0.03777803\n",
      "Iteration 180, loss = 0.03763037\n",
      "Iteration 181, loss = 0.03748494\n",
      "Iteration 182, loss = 0.03734168\n",
      "Iteration 183, loss = 0.03720054\n",
      "Iteration 184, loss = 0.03706147\n",
      "Iteration 185, loss = 0.03692441\n",
      "Iteration 186, loss = 0.03678931\n",
      "Iteration 187, loss = 0.03665612\n",
      "Iteration 188, loss = 0.03652481\n",
      "Iteration 189, loss = 0.03639531\n",
      "Iteration 190, loss = 0.03626759\n",
      "Iteration 191, loss = 0.03614160\n",
      "Iteration 192, loss = 0.03601731\n",
      "Iteration 193, loss = 0.03589467\n",
      "Iteration 194, loss = 0.03577364\n",
      "Iteration 195, loss = 0.03565420\n",
      "Iteration 196, loss = 0.03553629\n",
      "Iteration 197, loss = 0.03541989\n",
      "Iteration 198, loss = 0.03530496\n",
      "Iteration 199, loss = 0.03519147\n",
      "Iteration 200, loss = 0.03507939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amans\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classifier=MLPClassifier(hidden_layer_sizes=(4,3),activation='logistic',solver='sgd',learning_rate_init=0.5,random_state=1,verbose=True)\n",
    "classifier.fit(values_train,target_train)\n",
    "target_pred=classifier.predict(values_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "870410e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "870410e0",
    "outputId": "b8633174-a9ee-4d8a-bf01-c455a19ead12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[13  0  0]\n",
      " [ 0 12  0]\n",
      " [ 0  1 19]]\n",
      "Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "cm=confusion_matrix(target_test,target_pred)\n",
    "print('Confusion Matrix:',cm,sep='\\n')\n",
    "acc=accuracy_score(target_test,target_pred)\n",
    "print('Accuracy:',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b01a7fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b01a7fd",
    "outputId": "3d5b7637-0006-40a4-c1c8-043875a2d823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.32099564  0.17654544  0.25054482 -0.09405169]\n",
      " [-1.25790236 -1.66177616  0.30178706  0.00949894]\n",
      " [ 1.2543997   0.95327325 -1.73474442 -1.50693998]\n",
      " [ 1.22248208  1.51504431 -1.82394321 -1.49319046]]\n",
      "[[ 2.07703809  3.21131536 -2.11115077]\n",
      " [ 1.81400656  4.40002106 -1.71836159]\n",
      " [-3.96667654 -3.04795268  2.97751226]\n",
      " [-2.79249575 -0.71500416  2.52799742]]\n",
      "[[-2.14466447 -2.93947447  5.47255662]\n",
      " [-7.96868391  3.48845566  4.77709361]\n",
      " [ 3.40457888  1.77668435 -4.81448304]]\n"
     ]
    }
   ],
   "source": [
    "for i in classifier.coefs_:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "BackPropogation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
